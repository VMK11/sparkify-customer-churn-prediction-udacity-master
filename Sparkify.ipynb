{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install missingno\n",
    "# !pip install -U ppscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Wrangling\n",
    "import pandas                   as pd\n",
    "import numpy                    as np\n",
    "import pyspark\n",
    "from pyspark                    import SparkConf\n",
    "from pyspark.sql                import SparkSession, Window\n",
    "from pyspark.sql.functions      import isnan, when, count, col, isnull, udf, last, \\\n",
    "                                       first, struct, lit, approx_count_distinct, from_unixtime, \\\n",
    "                                    month, datediff, first, year, concat, substring,max as Fmax\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types          import IntegerType, DoubleType, StringType, TimestampType, DateType, LongType\n",
    "from pyspark.ml.feature         import StringIndexer, VectorAssembler, OneHotEncoderEstimator\n",
    "\n",
    "# Machine Learning\n",
    "from pyspark.ml.classification  import LogisticRegression\n",
    "from pyspark.ml.classification  import RandomForestClassifier\n",
    "from pyspark.ml.classification  import GBTClassifier\n",
    "from pyspark.ml.classification  import LinearSVC\n",
    "from pyspark.ml.classification  import DecisionTreeClassifier\n",
    "from pyspark.ml.classification  import NaiveBayes\n",
    "from pyspark.ml.evaluation      import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning          import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature         import StringIndexer, OneHotEncoderEstimator, MinMaxScaler, VectorAssembler\n",
    "from pyspark.ml                 import Pipeline\n",
    "import ppscore                  as pps\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier, NaiveBayes, RandomForestClassifier\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, Normalizer, StandardScaler, MinMaxScaler, IndexToString\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.tuning import CrossValidatorModel\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Visualizations\n",
    "from matplotlib.ticker          import PercentFormatter\n",
    "import matplotlib.pyplot        as plt\n",
    "import seaborn                  as sns\n",
    "\n",
    "# Miscellaneous \n",
    "from datetime                   import datetime\n",
    "from datetime                   import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# As suggested at \n",
    "# https://stackoverflow.com/questions/38417441/pyspark-socket-timeout-exception-after-application-running-for-a-while\n",
    "# increase Spark heartbeatInterval to prevent session from \"dying\"\n",
    "spark.conf.set(\"spark.executor.heartbeatInterval\",\"72000s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# Read the dataframe\n",
    "df = spark.read.json('mini_sparkify_event_data.json')\n",
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data schema to get an idea of what the data looks like\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts\n",
    "print(f'Total No. of records: {df.count()}')\n",
    "print(f'Total No. of Columns: {len(df.columns)}')\n",
    "print(f'No. of unique users: {df.select(\"userId\").distinct().count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check of Null's - Examine nature of creation for those nulls\n",
    "nulls_check = pd.DataFrame(df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).collect(),\n",
    "                           columns = df.columns).transpose()\n",
    "nulls_check.columns = ['Null Values']\n",
    "nulls_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "%matplotlib inline\n",
    "df_pandas = df.toPandas()\n",
    "msno.heatmap(df_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(df_pandas.isnull(), cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above count, we observe that:\n",
    "\n",
    "1. There is missing information for users not logged in, corresponding to empty userID.\n",
    "2. There are a lot of missing values in song, length, artist columns which may correspond to the events where the user is not listening to music."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    '''\n",
    "    Function which performs basic data cleaning.\n",
    "    \n",
    "    INPUT: \n",
    "    df - pyspark dataframe containing user events\n",
    "    \n",
    "    OUTPUT:\n",
    "    df_new - pyspark dataframe with removed rows with empty 'userId' column and duplicates if any\n",
    "    '''\n",
    "    \n",
    "    # remove rows where userId is empty and duplicated rows\n",
    "    df_clean = df.filter(df[\"userId\"] != \"\").dropDuplicates()\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "# Remove empy and duplicated records.\n",
    "df_clean = clean_data(df)\n",
    "\n",
    "# print out first line\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type Casting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string timestamp to date \n",
    "\n",
    "def get_date_from_timestamp(df, col_name, new_col_name):\n",
    "    return df.withColumn(new_col_name, F.to_timestamp(F.col(col_name) / 1000).astype(StringType()))\n",
    "\n",
    "# convert event timestamp\n",
    "df_clean = get_date_from_timestamp(df_clean, 'ts', 'ts_date')\n",
    "\n",
    "# convert registration timestamp\n",
    "df_clean = get_date_from_timestamp(df_clean, 'registration', 'registration_date')\n",
    "\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time period for all events in dataset\n",
    "df_clean.groupBy(year('registration_date'), month('registration_date')).count().sort(month('registration_date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time period for all events in dataset\n",
    "df_clean.groupBy(year('ts_date'), month('ts_date')).count().sort(month('ts_date')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above count, we observe that:\n",
    "\n",
    "1. The registrations at the service in the sepcific dataset happened between March & November.\n",
    "2. Most of the events in the dataset happened between October and November 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events.\n",
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_interaction_df = df.groupBy('gender').count().toPandas()\n",
    "gender_interaction_df['count'] = gender_interaction_df['count']/df.count()\n",
    "gender_interaction_df.plot.bar(x = 'gender', color = sns.color_palette()[0], legend = None)\n",
    "sns.set(style=\"ticks\")\n",
    "sns.despine(right=True,top=True)\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Share of Interactions')\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.savefig('sparkify_gender_interaction.png', dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at gender distribution by user, i.e. how many users identified as male, female or did not provide an answer?\n",
    "auth_df = df.groupBy(['auth']).count().toPandas()\n",
    "auth_df = auth_df['auth'].value_counts(normalize = True, dropna = False)\n",
    "auth_df.plot.bar(x=auth_df.index, color = sns.color_palette()[0])\n",
    "sns.set(style=\"ticks\")\n",
    "sns.despine(right=True,top=True)\n",
    "plt.xlabel('Proportion')\n",
    "plt.ylabel('Share of Users')\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.savefig('sparkify_auth_types.png', dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at gender distribution by user, i.e. how many users identified as male, female or did not provide an answer?\n",
    "level_df = df.groupBy(['level']).count().toPandas()\n",
    "level_df = level_df['level'].value_counts(normalize = True, dropna = False)\n",
    "level_df.plot.bar(x=level_df.index, color = sns.color_palette()[0])\n",
    "sns.set(style=\"ticks\")\n",
    "sns.despine(right=True,top=True)\n",
    "plt.xlabel('Users')\n",
    "plt.ylabel('Proportion')\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.savefig('sparkify_level_types.png', dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics with regards to all user interactions history\n",
    "interactions_df = df.groupBy('userId').count().toPandas()\n",
    "interactions_df.plot.hist(sns.color_palette()[0], legend = None)\n",
    "plt.xlabel('Number of Interactions')\n",
    "sns.set(style=\"ticks\")\n",
    "sns.despine(right=True,top=True)\n",
    "plt.savefig('sparkify_num_interactions.png', dpi = 300)\n",
    "\n",
    "print('Median number of interactions per user: {}'.format(interactions_df['count'].median()))\n",
    "print('Standard deviation of interactions per user: {}'.format(interactions_df['count'].std()))\n",
    "print('Minimum number of interactions for a single user: {}'.format(interactions_df['count'].min()))\n",
    "print('Maximum number of interactions for a single user: {}'.format(interactions_df['count'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = df_clean.select('userId').distinct().count()\n",
    "print(f'There are {df_clean.count()} events recorded in the small subset of data generated by {num_users} unique users.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column Churn when the event recorded is 'Cancellation Confirmation'. \n",
    "\n",
    "# A value equal to 1 indicates that the user cancelled the subscription.\n",
    "churn_cancellation = udf(lambda x: 1 if x==\"Cancellation Confirmation\" else 0, IntegerType())\n",
    "\n",
    "df_clean = df_clean.withColumn(\"churn_cancellation\", churn_cancellation(\"page\"))\n",
    "df_clean.where(df_clean['churn_cancellation']==1).head(1)\n",
    "\n",
    "cancelled_users = df_clean.select(['userId']).where(df_clean.churn_cancellation == 1).groupby('userId').count().toPandas()['userId'].values\n",
    "print(f'There are {len(cancelled_users)} users that cancelled the subscription.')\n",
    "# set column 'churn_cancellation' to 1 for all rows for cancelled_users\n",
    "\n",
    "df_clean = df_clean.withColumn('churn_cancellation', when((df_clean.userId).isin(list(cancelled_users)), 1).otherwise(0))\n",
    "\n",
    "# Show counts per value for 'churn_cancellation'\n",
    "df_clean.groupBy('churn_cancellation').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check for some users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(cancelled_users))\n",
    "# show all events related to a churned user\n",
    "df_clean.select(['firstName', 'level', 'method', 'page', 'ts_date','churn_cancellation']).where(df_clean.userId == '129').toPandas().head(n=5)\n",
    "df_clean.select(['firstName', 'level', 'method', 'page', 'ts_date','churn_cancellation']).where(df_clean.userId == '99').toPandas().head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_active_users = df_clean.where(df_clean.churn_cancellation == 0).groupBy('userId').count().count()\n",
    "print(f'There are {num_active_users} active users and {len(cancelled_users)} users who cancelled the subscription.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chrun Rate: Gender Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of female and male users\n",
    "male = df_clean.select('userId', 'gender').where(df.gender == 'M').groupBy('userId').count().agg(count(\"count\"))\n",
    "female = df_clean.select('userId', 'gender').where(df.gender == 'F').groupBy('userId').count().agg(count(\"count\"))\n",
    "\n",
    "print(f'There are {male.collect()[0][\"count(count)\"]} male and {female.collect()[0][\"count(count)\"]} female users in the dataset.')\n",
    "      \n",
    "male_churned_perc = df_clean.where((df_clean.gender == 'M') & (df_clean.churn_cancellation == 1)).groupBy('userId').count().count() / df_clean.where(df_clean.churn_cancellation == 1).groupBy('userId', 'gender').count().count() * 100\n",
    "print(f'{round(male_churned_perc)}% of users who churned are male.')\n",
    "      \n",
    "male_active_perc = df_clean.where((df_clean.gender == 'M') & (df_clean.churn_cancellation == 0)).groupBy('userId').count().count() / df_clean.where(df_clean.churn_cancellation == 0).groupBy('userId', 'gender').count().count() * 100\n",
    "print(f'{round(male_active_perc)}% of users who are active are male.')\n",
    "      \n",
    "female_active_perc = df_clean.where((df_clean.gender == 'F') & (df_clean.churn_cancellation == 0)).groupBy('userId').count().count() / df_clean.where(df_clean.churn_cancellation == 0).groupBy('userId', 'gender').count().count() * 100\n",
    "print(f'{round(female_active_perc)}% of users who are active are female.')\n",
    "      \n",
    "female_users_churned_perc = df_clean.where((df_clean.gender == 'F') & (df_clean.churn_cancellation == 1)).groupBy('userId').count().count() / df_clean.where(df_clean.gender == 'F').groupBy('userId').count().count() * 100\n",
    "print(f'Among female users {round(female_users_churned_perc)}% churned.')\n",
    "      \n",
    "male_users_churned_perc = df_clean.where((df_clean.gender == 'M') & (df_clean.churn_cancellation == 1)).groupBy('userId').count().count() / df_clean.where(df_clean.gender == 'M').groupBy('userId').count().count() * 100\n",
    "print(f'Among male users {round(male_users_churned_perc)}% churned.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above comparison, we observe that:\n",
    "\n",
    "1. There are more male users (121) than female users (104).\n",
    "2. Most of the users who have churned are male (62%).\n",
    "3. Among female users 19% have churned, while among male users 26% have churned.\n",
    "4. Female users seem to be more active. They have generated 56% of the events.\n",
    "5. Female users who churned generated fewer events than male users who churned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chrun Rate: Plan Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of paid and free accounts\n",
    "\n",
    "paid = df_clean.select('userId', 'level').where(df_clean.level == 'paid').groupBy('userId').count().agg(count(\"count\"))\n",
    "free = df_clean.select('userId', 'level').where(df_clean.level == 'free').groupBy('userId').count().agg(count(\"count\"))\n",
    "\n",
    "print(f'There are {paid.collect()[0][\"count(count)\"]} paid and {free.collect()[0][\"count(count)\"]} free plan accounts in dataset.')\n",
    "      \n",
    "paid_churned = df_clean.where((df_clean.level == 'paid') & (df_clean.churn_cancellation == 1)).groupBy('userId').count().count() / df_clean.where(df_clean.churn_cancellation == 1).groupBy('userId', 'level').count().count() * 100\n",
    "print(f'{round(paid_churned)}% of churned users were on a paid plan.')\n",
    "      \n",
    "paid_active = df_clean.where((df_clean.level == 'paid') & (df_clean.churn_cancellation == 0)).groupBy('userId').count().count() / df_clean.where(df_clean.churn_cancellation == 0).groupBy('userId', 'level').count().count() * 100\n",
    "print(f'{round(paid_active)}% of active users are on a paid plan.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above comparison, we observe that:\n",
    "\n",
    "1. There is not a big difference between active and cancelled users in terms of the proportion of paid subscription plans. Overall, there are more free subscriptions than paid ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Churn Rate: Location Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column 'city'\n",
    "df_clean = df_clean.withColumn(\"temp_city\", F.split(df_clean.location, ',').getItem(0))\n",
    "df_clean = (df_clean.withColumn(\"city\", F.split(df_clean.temp_city, '-').getItem(0)) \n",
    "                    .drop('temp_city'))\n",
    "\n",
    "# Create column 'state'\n",
    "df_clean = df_clean.withColumn(\"state\", substring(df_clean.location, -2, 2))\n",
    "\n",
    "# Create column with last location of the user\n",
    "# w = Window.partitionBy('userId')\n",
    "# df_clean = df_clean.withColumn('last_ts', max('ts_date').over(w))\n",
    "# df_clean = df_clean.withColumn('last_city', when(df_clean.last_ts == df_clean.ts_date, df_clean.city))\n",
    "# df_clean = df_clean.withColumn('last_state', when(df_clean.last_ts == df_clean.ts_date, df_clean.state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_location = df_clean.select('userId','churn_cancellation', 'city', 'state').toPandas()\n",
    "df_location_pd = df_location.groupby(['userId', 'state']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of events by state\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "x, y, hue = 'state', 'proportion', 'churn_cancellation'\n",
    "hue_order = ['Active users', 'Churned users']\n",
    "\n",
    "(df_location[x]\n",
    " .groupby(df_location[hue])\n",
    " .value_counts(normalize=True)\n",
    " .rename(y)\n",
    " .reset_index()\n",
    " .pipe((sns.barplot, 'data'), x=x, y=y, hue=hue))\n",
    "\n",
    "plt.legend(loc='upper right', title='Churn')\n",
    "plt.title('Location Distribution of Events')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Proportion of events')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above comparision we observe that:\n",
    "\n",
    "1. It appears to be differences in the distribution between active and churned users across states. In addition, there are some states that have no churned users (e.g.New Hampshire), and some states that have only churned users (e.g., Arkansas, Oregon).\n",
    "\n",
    "2. California, Pennsylvania, Texas, Florida have the most users and events for active and churned users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_explore(df):\n",
    "    \"\"\"\n",
    "    Function that calculates the feature correlation & predictive importance\n",
    "    \n",
    "    INPUT:\n",
    "    df - Pandas dataframe containing Sparkify events\n",
    "    \n",
    "    \n",
    "    OUTPUT:\n",
    "    Seaborn Heatmaps\n",
    "    \n",
    "    \"\"\"\n",
    "    # Check correlation among features\n",
    "\n",
    "    # creating mask\n",
    "    mask = np.triu(np.ones_like(df.corr()))\n",
    "\n",
    "    # plotting a triangle correlation heatmap\n",
    "    dataplot = sns.heatmap(df.corr(), cmap=\"YlGnBu\", annot=True, mask=mask)\n",
    "    \n",
    "    \n",
    "    # Check predictive importance amongst features\n",
    "    matrix = pps.matrix(df)\n",
    "    sns.heatmap(matrix,annot=True,fmt=\".2f\")\n",
    "    \n",
    "    \n",
    "    # displaying heatmap\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "def clean_data(df):\n",
    "    '''\n",
    "    Function that performs data cleaning of Sparkify dataset.\n",
    "    \n",
    "    INPUT: \n",
    "    df - pyspark dataframe containing Sparkify events\n",
    "    \n",
    "    OUTPUT:\n",
    "    df_new - pyspark dataframe with removed rows with empty 'userId' column and duplicates if any\n",
    "    '''\n",
    "    \n",
    "    # remove rows where userId is empty and duplicated rows\n",
    "    df_clean = df.filter(df[\"userId\"] != \"\").dropDuplicates()\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "def get_date_from_timestamp(df, col_name, new_col_name):\n",
    "    '''\n",
    "    Function that convert timestamp to date\n",
    "    '''\n",
    "    \n",
    "    return df.withColumn(new_col_name, F.to_timestamp(F.col(col_name) / 1000).astype(StringType()))\n",
    "\n",
    "\n",
    "def prepare_dataset(df):\n",
    "    '''\n",
    "    Function for preparation of dataset for machine learning models.\n",
    "    \n",
    "    INPUT:\n",
    "    df - initial dataset loaded from json file\n",
    "    \n",
    "    OUTPUT:\n",
    "    df_ml - new dataset prepared for machine learning which\n",
    "    contains the following columns:\n",
    "    \n",
    "    1. userId - initial id of the user\n",
    "    2. gender - user's gender\n",
    "    3. days_registered - days since user's registration\n",
    "    4. last_state - current state of the user\n",
    "    5. avg_songs_per_day - average songs played per day\n",
    "    6. last_level - current subscription plan\n",
    "    7. Thumbsup_proportion - ratio of thumbs up over thumbs down\n",
    "    8. num_add_friends - number of add friends events\n",
    "    9. avg_roll_adv_per_day - average number of roll adv played per day\n",
    "    10. churn_cancellation\n",
    "    '''\n",
    "    \n",
    "    # Clean dataset using clean_data function\n",
    "    df = clean_data(df)\n",
    "    \n",
    "    # Convert event timestamp\n",
    "    df = get_date_from_timestamp(df, 'ts', 'ts_date')\n",
    "    \n",
    "    # Convert registration timestamp\n",
    "    df = get_date_from_timestamp(df, 'registration', 'registration_date')\n",
    "    \n",
    "    # Create column Churn when the event recorded is 'Cancellation Confirmation'. \n",
    "    # A value equal to 1 indicates that the user cancelled the subscription.\n",
    "    churn_cancellation = udf(lambda x: 1 if x==\"Cancellation Confirmation\" else 0, IntegerType())\n",
    "    \n",
    "    df = df.withColumn(\"churn_cancellation\", churn_cancellation(\"page\"))\n",
    "    \n",
    "    # Get userId with churn_cancellation == 1\n",
    "    cancelled_users = df.select(['userId']).where(df.churn_cancellation == 1).groupby('userId').count().toPandas()['userId'].values\n",
    "    cancelled_users = list(cancelled_users)\n",
    "    df = df.withColumn('churn_cancellation', when((df.userId).isin(cancelled_users), 1).otherwise(0))\n",
    "    \n",
    "    # Convert column gender to numeric: 1 for 'female' and 0 for 'male'\n",
    "    gender = udf(lambda x: 1 if x==\"F\" else 0, IntegerType())\n",
    "    \n",
    "    df = df.withColumn(\"gender\", gender(\"gender\"))\n",
    "    \n",
    "    # Convert column level to numeric: 1 for 'paid' and 0 for 'free'\n",
    "    level = udf(lambda x: 1 if x==\"paid\" else 0, IntegerType())\n",
    "    \n",
    "    #df = df.withColumn(\"level\", level(\"level\"))\n",
    "    \n",
    "    levels = df.select(['userId', 'level', 'ts_date'])\\\n",
    "                        .orderBy(desc('ts_date'))\\\n",
    "                        .dropDuplicates(['userId'])\\\n",
    "                        .select(['userId', 'level'])\\\n",
    "                        .withColumn('last_level', level('level').cast(IntegerType()))\n",
    "    levels = levels.drop('level')\n",
    "    levels = levels.withColumnRenamed('userId', 'level_userId')\n",
    "\n",
    "    \n",
    "    # Compute active days as number of days since registration\n",
    "    # Create dataframe with last timestamp (ts_date) per user\n",
    "    df_last_ts = df.groupBy('userId').agg(max('ts_date').alias('last_interaction'))\n",
    "    \n",
    "    # Join dataframes and add column 'days_registered' with number of days from registration date to ts for last event\n",
    "    df = df.join(df_last_ts, on='userId').select(df_last_ts['*'], df['*']).withColumn('days_registered', datediff(df_last_ts['last_interaction'], df['registration_date']).cast('float'))\n",
    "    \n",
    "    # Create a new column 'date' with format 'yyyy-MM-dd'\n",
    "    df = df.withColumn('date', date_format('ts_date', 'yyyy-MM-dd').alias('date'))\n",
    "    \n",
    "    # Create a new column 'last_state' where the last session was recorded\n",
    "    df = df.withColumn(\"state\", substring(df.location, -2, 2))\n",
    "    df = df.withColumn('last_state', when(df_last_ts.last_interaction == df.ts_date, df.state))\n",
    "        \n",
    "    # Compute average songs played by day per user\n",
    "    w = Window.partitionBy('userId', 'date')\n",
    "    songs = df.where(df.page == 'NextSong').select('userId', 'date', count('userId').over(w).alias('songs')).distinct()\n",
    "    w = Window.partitionBy('userId')\n",
    "    songs = songs.withColumn('avg_songs_per_day', avg('songs').over(w))\n",
    "    songs = songs.select(songs['userId'].alias('songs_userId'), 'avg_songs_per_day')\n",
    "    songs = songs.withColumn('avg_songs_per_day', F.round(songs['avg_songs_per_day'],2)).distinct()\n",
    "    \n",
    "    # Compute the number of thumbs up for user\n",
    "    w = Window.partitionBy('userId')\n",
    "    thumbs_up = df.where(df.page == 'Thumbs Up').select('userId', count('userId').over(w).alias('thumbs_up')).distinct()\n",
    "    thumbs_up = thumbs_up.select(thumbs_up['userId'].alias('thumbsup_userId'), 'thumbs_up')\n",
    " \n",
    "    # Compute the number of thumbs down per user\n",
    "    w = Window.partitionBy('userId')\n",
    "    thumbs_down = df.where(df.page == 'Thumbs Down').select('userId', count('userId').over(w).alias('thumbs_down')).distinct()\n",
    "    thumbs_down = thumbs_down.select(thumbs_down['userId'].alias('thumbsdown_userId'), 'thumbs_down')\n",
    "      \n",
    "    # Compute the number of add friend events per user\n",
    "    w = Window.partitionBy('userId')\n",
    "    num_add_friend = df.where(df.page == 'Add Friend').select('userId', count('userId').over(w).alias('num_add_friend')).distinct()\n",
    "    num_add_friend = num_add_friend.select(num_add_friend['userId'].alias('friends_userId'), 'num_add_friend')\n",
    "    \n",
    "    # Compute the fraction of page 'roll advert'\n",
    "    w = Window.partitionBy('userId', 'date')\n",
    "    roll_adv = df.where(df.page == 'Roll Advert').select('userId', 'date', count('userId').over(w).alias('roll_adv')).distinct()\n",
    "    w = Window.partitionBy('userId')\n",
    "    roll_adv = roll_adv.withColumn('avg_roll_adv_per_day', avg('roll_adv').over(w))\n",
    "    roll_adv = roll_adv.select(roll_adv['userId'].alias('rolladv_userId'), 'avg_roll_adv_per_day')\n",
    "    roll_adv = roll_adv.withColumn('avg_roll_adv_per_day', F.round(roll_adv['avg_roll_adv_per_day'],2)).distinct()\n",
    "    \n",
    "    # Construct the final dataset\n",
    "    df_ml = df.select('userId', 'gender', 'churn_cancellation', 'days_registered', 'last_state').dropna().drop_duplicates()\n",
    "    df_ml = df_ml.join(songs, df_ml.userId == songs.songs_userId, how='left').distinct() \n",
    "    df_ml = df_ml.join(levels, df_ml.userId == levels.level_userId, how='left').distinct()\n",
    "    df_ml = df_ml.join(thumbs_up, df_ml.userId == thumbs_up.thumbsup_userId, how='left').distinct()\n",
    "    df_ml = df_ml.fillna(0, subset=['thumbs_up'])\n",
    "    df_ml = df_ml.join(thumbs_down, df_ml.userId == thumbs_down.thumbsdown_userId, how='left').distinct()\n",
    "    df_ml = df_ml.fillna(0, subset=['thumbs_down'])\n",
    "    df_ml = df_ml.withColumn('Thumbsup_proportion', F.round(df_ml.thumbs_up / df_ml.thumbs_down, 2))\n",
    "    df_ml = df_ml.fillna(0, subset=['Thumbsup_proportion'])\n",
    "    df_ml = df_ml.join(num_add_friend, df_ml.userId == num_add_friend.friends_userId, how='left').distinct()\n",
    "    df_ml = df_ml.fillna(0, subset=['num_add_friend'])\n",
    "    df_ml = df_ml.join(roll_adv, df_ml.userId == roll_adv.rolladv_userId, how='left').distinct()\n",
    "    df_ml = df_ml.fillna(0, subset=['avg_roll_adv_per_day'])\n",
    "    df_ml = df_ml.drop('thumbs_up', 'thumbs_down', 'songs_userId', 'level_userID', 'rolladv_userId', 'thumbsup_userId', 'thumbsdown_userId', 'friends_userId')\n",
    "\n",
    "    return df_ml\n",
    "\n",
    "mini_sparkify_event_data = 'mini_sparkify_event_data.json'\n",
    "df = spark.read.json(mini_sparkify_event_data)\n",
    "df.persist()\n",
    "\n",
    "df_ml = prepare_dataset(df)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Correlation & Predictive Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_explore(df_ml.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluator User Defined Functions\n",
    "def udfModelEvaluator(dfPredictions, labelColumn='label'):\n",
    "\n",
    "    colSelect = dfPredictions.select(\n",
    "      [F.col('prediction').cast(DoubleType())\n",
    "       ,F.col(labelColumn).cast(DoubleType()).alias('label')])\n",
    "\n",
    "    metrics = MulticlassMetrics(colSelect.rdd)\n",
    "\n",
    "    mMatrix = metrics.confusionMatrix().toArray().astype(int)    \n",
    "\n",
    "    mTP = metrics.confusionMatrix().toArray()[1][1]\n",
    "    mTN = metrics.confusionMatrix().toArray()[0][0]\n",
    "    mFP = metrics.confusionMatrix().toArray()[0][1]\n",
    "    mFN = metrics.confusionMatrix().toArray()[1][0]\n",
    "    \n",
    "    mAccuracy = metrics.accuracy\n",
    "    mPrecision = mTP / (mTP + mFP)\n",
    "    mRecall = mTP / (mTP + mFN)\n",
    "    mF1 = metrics.fMeasure(1.0)\n",
    "\n",
    "    mResults = [mAccuracy, mPrecision, mRecall, mF1, mMatrix, mTP, mTN, mFP, mFN, \"Return [[0]=Accuracy, [1]=Precision, [2]=Recall, [3]=F1, [4]=ConfusionMatrix, [5]=TP, [6]=TN, [7]=FP, [8]=FN]\"]\n",
    "\n",
    "    return mResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets (80% - 20%)\n",
    "\n",
    "df_ml = df_ml.withColumnRenamed(\"churn_cancellation\", \"label\")\n",
    "\n",
    "train, test = df_ml.randomSplit([0.8, 0.2], seed = 42)\n",
    "\n",
    "# Count users by label in train set\n",
    "\n",
    "counts_train = train.groupBy('label').count().toPandas()\n",
    "print(counts_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking 70% of both 0's and 1's into training set\n",
    "train = df_ml.sampleBy(\"label\", fractions={0: 0.7, 1: 0.7}, seed=10)\n",
    "test = df_ml.subtract(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts\n",
    "\n",
    "count_churned = counts_train[counts_train['label']==1]['count'].values[0]\n",
    "count_total = counts_train['count'].sum()\n",
    "\n",
    "# Weights\n",
    "\n",
    "c = 2 # number of labels\n",
    "weight_churned = count_total / (c * count_churned)\n",
    "weight_no_churned = count_total / (c * (count_total - count_churned))\n",
    "\n",
    "# Create a new column 'weight' containing a weight for each observation according to its class (i.e. churned / not churned)\n",
    "\n",
    "train = train.withColumn(\"weight\", when(train.label ==1, weight_churned).otherwise(weight_no_churned))\n",
    "train.select('label', 'weight').where(train.label ==1).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index and encode categorical feature 'last_state'\n",
    "stringIndexerState = StringIndexer(inputCol=\"last_state\", \n",
    "                                   outputCol=\"stateIndex\", \n",
    "                                   handleInvalid = 'skip')\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"stateIndex\",\n",
    "                        outputCol=\"stateVec\",\n",
    "                        )\n",
    "\n",
    "# Create a vector for features to be used in the models\n",
    "features = ['stateVec', 'gender', 'days_registered', 'avg_songs_per_day', 'last_level', 'Thumbsup_proportion', 'num_add_friend', 'avg_roll_adv_per_day']\n",
    "\n",
    "# Merge multiple columns into a vector column\n",
    "assemblers = VectorAssembler(inputCols=features, outputCol=\"rawFeatures\")\n",
    "\n",
    "# Scale features\n",
    "scalers = MinMaxScaler(inputCol=\"rawFeatures\", outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluator User Defined Functions\n",
    "def udfModelEvaluator(dfPredictions, labelColumn='label'):\n",
    "\n",
    "    colSelect = dfPredictions.select(\n",
    "      [F.col('prediction').cast(DoubleType())\n",
    "       ,F.col(labelColumn).cast(DoubleType()).alias('label')])\n",
    "\n",
    "    metrics = MulticlassMetrics(colSelect.rdd)\n",
    "\n",
    "    mMatrix = metrics.confusionMatrix().toArray().astype(int)    \n",
    "\n",
    "    mTP = metrics.confusionMatrix().toArray()[1][1]\n",
    "    mTN = metrics.confusionMatrix().toArray()[0][0]\n",
    "    mFP = metrics.confusionMatrix().toArray()[0][1]\n",
    "    mFN = metrics.confusionMatrix().toArray()[1][0]\n",
    "    \n",
    "    mAccuracy = metrics.accuracy\n",
    "    mPrecision = mTP / (mTP + mFP)\n",
    "    mRecall = mTP / (mTP + mFN)\n",
    "    mF1 = metrics.fMeasure(1.0)\n",
    "\n",
    "    mResults = [mAccuracy, mPrecision, mRecall, mF1, mMatrix, mTP, mTN, mFP, mFN, \"Return [[0]=Accuracy, [1]=Precision, [2]=Recall, [3]=F1, [4]=ConfusionMatrix, [5]=TP, [6]=TN, [7]=FP, [8]=FN]\"]\n",
    "\n",
    "    return mResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_duration (t_dif):\n",
    "    t_s = t_dif.seconds\n",
    "    duration = {}\n",
    "    duration['h'], rem = divmod(t_s, 3600) \n",
    "    duration['m'], duration['s'] = divmod(rem, 60)\n",
    "    stamp = ''\n",
    "    if duration['h']>0:\n",
    "        stamp += f\"{duration['h']} hour(s), \" \n",
    "    if duration['m']>0:\n",
    "        stamp += f\"{duration['m']} minute(s) and \"\n",
    "    # seconds and fraction of seconds\n",
    "    frac = int(t_dif.microseconds/10000)/100\n",
    "    stamp += f\"{duration['s'] + frac} second(s)\"\n",
    "    # print(f\"{duration['h']}h:{duration['m']}m:{duration['s']}s\")\n",
    "    return stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_formated_metrics(selected_model, test_data):\n",
    "    '''\n",
    "    Prints a compacted dataframe with all the model's metrics\n",
    "    selected_model: The fitted model\n",
    "    test_data: the test data portion\n",
    "    '''\n",
    "    def get_model_metrics(selected_model, model_type = 'train'):\n",
    "        '''\n",
    "        Get the metrics of a model\n",
    "        selected_model:  the fitted model\n",
    "        model_type: either 'train' (default) or 'test'\n",
    "        '''\n",
    "        if model_type == 'train':\n",
    "            metrics = selected_model.bestModel.summary\n",
    "        else: \n",
    "            metrics = selected_model\n",
    "        acc = metrics.accuracy, \n",
    "        general = np.array((metrics.weightedFMeasure(),\n",
    "                   metrics.weightedPrecision, metrics.weightedRecall,\n",
    "                   metrics.weightedTruePositiveRate, metrics.weightedFalsePositiveRate))\n",
    "        general = general.reshape(1, general.shape[0])\n",
    "        labels = ['General'] + [f'Churn={x}' for x in metrics.labels]\n",
    "        labeled = np.array((metrics.fMeasureByLabel(),\n",
    "                          metrics.precisionByLabel, metrics.recallByLabel,\n",
    "                          metrics.truePositiveRateByLabel, metrics.falsePositiveRateByLabel))\n",
    "        conc_results = np.concatenate((general.T, labeled), axis=1)\n",
    "        metrics_names = ['F-Measure', 'Precision', 'Recall', 'True_+ve_Rate', 'False_+ve_Rate']\n",
    "        df_res = pd.DataFrame(conc_results, columns=labels, index=metrics_names)\n",
    "        return acc[0], df_res\n",
    "    \n",
    "    # Apply for training data\n",
    "    acc_train, train_res = get_model_metrics(selected_model)\n",
    "    # Get the results of the test data\n",
    "    model_test = selected_model.bestModel.evaluate(test_data)\n",
    "    # Apply on test data\n",
    "    acc_test, test_res = get_model_metrics(model_test, model_type='test')\n",
    "    \n",
    "    # Concatenate to a pretty dataframe\n",
    "    pretty_frame = pd.concat([train_res, test_res], axis=1, keys=[\n",
    "                    f'Training (Accuracy = {acc_train*100:4.2f}%)',\n",
    "                    f'Testing (Accuracy = {acc_test*100:4.2f}%)'])\n",
    "    return pretty_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_features_contribution(fitted_model, x_labels, scale_to='full_range'):\n",
    "    '''\n",
    "    Draws a bar chart of features vs churn %\n",
    "    fitted_model: the fitted model\n",
    "    scale_to: the values will be scated to:\n",
    "           'full_range' where the full absolute values are summed to 100.\n",
    "           'maximum_range' where the maximum absolute extremes are scalled to 100.\n",
    "           'none' the values are shown as is.\n",
    "    '''\n",
    "    cmx = fitted_model.bestModel.coefficientMatrix\n",
    "    cmv = cmx.values\n",
    "    # cmv.shape, len(final_df.columns[2:])\n",
    "    \n",
    "    # Define positive and negative values\n",
    "    positives_v = np.array([x if x>=0 else 0 for x in cmv])\n",
    "    negatives_v = np.array([x if x<=0 else 0 for x in cmv])    \n",
    "    \n",
    "    # Drawing by scalling the maximum range to  100\n",
    "    if scale_to == 'full_range':\n",
    "        rang = positives_v.sum()+ abs(negatives_v).sum()\n",
    "    elif scale_to == 'maximum_range':\n",
    "        rang = positives_v.max()+ abs(negatives_v).max()\n",
    "    else:\n",
    "        rang = 1.\n",
    "        \n",
    "    positives_v /= rang\n",
    "    negatives_v /= rang\n",
    "    positives_v *= 100.\n",
    "    negatives_v *= 100.\n",
    "    fig, ax = plt.subplots(figsize=(16, 9))\n",
    "    ax.bar(x_labels, positives_v,color='r')\n",
    "    ax.bar(x_labels, negatives_v, color='g')\n",
    "    ax.set_xlabel('Features')\n",
    "    ax.set_ylabel('The user is most likely to churn (%)')\n",
    "    ax.set_title('Contribution of each feature to the churn decission')\n",
    "    ax.set_xticklabels(labels = final_df.columns[2:], rotation='vertical');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifier_metrics(trained_model, train_data, test_data):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    def get_specific_metrics(trained_model, data):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        res2 = trained_model.transform(data).select('label', 'prediction')\n",
    "        TruePos = res2.filter((res2.prediction==1)& (res2.label == res2.prediction) ).count()\n",
    "        TrueNeg = res2.filter((res2.prediction==0)& (res2.label == res2.prediction) ).count()\n",
    "        FalsPos = res2.filter((res2.prediction==1)& (res2.label != res2.prediction) ).count()\n",
    "        FalsNeg = res2.filter((res2.prediction==0)& (res2.label != res2.prediction) ).count()\n",
    "        accuracy = res2.filter(res2.label == res2.prediction).count()/res2.count()\n",
    "        precision = TruePos/(TruePos+FalsPos)\n",
    "        recall = TruePos/(TruePos+FalsNeg)\n",
    "        f1score = 2 * precision * recall / (precision + recall)\n",
    "        return accuracy, precision, recall, f1score\n",
    "    train_metrics = get_specific_metrics(trained_model, train_data)\n",
    "    test_metrics = get_specific_metrics(trained_model, test_data)\n",
    "    labels =['Train', 'Test']\n",
    "    metrics_names = ['Accuracy', 'Precision', 'Recall', 'F-Score']\n",
    "    metrics_data =np.array((train_metrics, test_metrics))\n",
    "    return pd.DataFrame(data=metrics_data.T, columns=labels, index=metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_features_importance(fitted_model, x_labels, threshold=0.1):\n",
    "    '''\n",
    "    Draws a pie chart of features\n",
    "    fitted_model: the fitted model\n",
    "    x_labels: the labels of the features.\n",
    "    threshold: the minimum value (%) to consider, \n",
    "               if the value is less than that, \n",
    "               it will be neglected (default =0)\n",
    "    '''\n",
    "    importance = list(fitted_model.bestModel.featureImportances.toArray())\n",
    "    # Get the threshold value\n",
    "    thres_v = threshold / 100 \n",
    "    # get the included and neglected values\n",
    "    active_values = [x for x in importance if x >= thres_v]\n",
    "    neglected = [x for x in importance if x < thres_v]\n",
    "    non_zero_neglected = [x for x in neglected if x > 0]\n",
    "    # print(importance, '\\n', x_labels, '\\n', thres_v, '\\n', \n",
    "    #       active_values, '\\n', neglected, '\\n', non_zero_neglected)\n",
    "    # get the accepted indexes\n",
    "    active_idx = [importance.index(x) for x in active_values]\n",
    "    # the accepted lables + minor features\n",
    "    active_labels = [x_labels[x] for x in active_idx]\n",
    "    minor_v = sum(neglected)\n",
    "    # print(active_idx, '\\n', active_labels, '\\n', minor_v)\n",
    "    # If there is any minor features\n",
    "    if minor_v>0:\n",
    "        active_values.append(minor_v)\n",
    "        active_labels.append(f'MINOR, ({len( \\\n",
    "                non_zero_neglected)}feats. each<{threshold}%)')\n",
    "    # print(active_labels, '\\n', active_values)\n",
    "    \n",
    "    # sorting\n",
    "    active_labels =[x for _, x in sorted(zip(active_values, active_labels))]\n",
    "    active_values = sorted(active_values)\n",
    "    \n",
    "    # Draw\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(16, 9))\n",
    "    ax.pie(active_values[::-1], labels=active_labels[::-1] , \n",
    "           autopct='%1.1f%%', shadow=True, \n",
    "           startangle=90 )\n",
    "    ax.set_title('Importance of each feature to the churn decission')\n",
    "    ax.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fitting(data, model_type, param_grid, save_as, num_folds=2, random_seed=179):\n",
    "    \"\"\"\n",
    "    Function that trains selected ML models and compares their performance.\n",
    "    \n",
    "    INPUT:\n",
    "    train - pyspark dataframe containing 80% of Sparkify events\n",
    "    test  - pyspark dataframe containing 20% of Sparkify events\n",
    "    OUTPUT:\n",
    "    model - pyspark ml object\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    model_evaluator = CrossValidator(estimator=model_type, estimatorParamMaps=param_grid,\n",
    "                                      evaluator=BinaryClassificationEvaluator(),\n",
    "                                      numFolds=num_folds, seed=random_seed, parallelism = 3)\n",
    "    t_start = pd.Timestamp.now()\n",
    "    print ('Fitting in progress...', end=' ')\n",
    "    fitted_model = model_evaluator.fit(data)\n",
    "    best_model = fitted_model.bestModel\n",
    "    t_dif = pd.Timestamp.now() - t_start\n",
    "    print (f'Done in {format_duration(t_dif)}')\n",
    "    t_start = pd.Timestamp.now()\n",
    "    print (f'\\nSaving the model as {save_as}...' , end=' ')\n",
    "    try:\n",
    "        best_model.save(save_as)\n",
    "    except:\n",
    "        # Overwrite if exists\n",
    "        best_model.write().overwrite().save(save_as)\n",
    "        print ('*Overwritten* ', end='')\n",
    "    t_dif = pd.Timestamp.now() - t_start\n",
    "    print (f'Done in {format_duration(t_dif)}')\n",
    "    \n",
    "    return fitted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Logistic Regression model\n",
    "model = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "pipeline_lr = Pipeline(stages=[stringIndexerState, encoder, assemblers, scalers, model])\n",
    "\n",
    "# param_grid = ParamGridBuilder() \\\n",
    "#     .addGrid(model.regParam,[0.01, 0.1]) \\\n",
    "#     .addGrid(model.elasticNetParam,[0.0, 0.5]) \\\n",
    "#     .addGrid(model.aggregationDepth,[2, 5]) \\\n",
    "#     .build()\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(model.regParam,[0.01]) \\\n",
    "    .build()\n",
    "\n",
    "m = model_fitting(train, pipeline_lr, param_grid, './models/LogisticRegression.mdl')\n",
    "# Model metrics\n",
    "display(get_formated_metrics(m, test))\n",
    "\n",
    "# Confusion Matrix\n",
    "pred_train = m.transform(train)\n",
    "pred_test = m.transform(test)\n",
    "\n",
    "predictionAndLabels = pred_train.rdd.map(lambda lp: (float(lp.prediction), float(lp.label))).toDF().withColumnRenamed('_1', 'prediction').withColumnRenamed('_2', 'label')\n",
    "metricsList_train_lr = udfModelEvaluator(predictionAndLabels, \"label\")\n",
    "\n",
    "ax=plt.subplot()\n",
    "sns.heatmap(metricsList_train_lr[4], annot=True, fmt='g', cmap='Blues', ax=ax)\n",
    "ax.xaxis.set_ticklabels(['Not Churned', 'Churned'])\n",
    "ax.yaxis.set_ticklabels(['Not Churned', 'Churned'])\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Features effect\n",
    "#draw_features_contribution(m, x_labels=features_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
